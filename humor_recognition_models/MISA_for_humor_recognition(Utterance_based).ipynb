{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MISA for humor recognition(Utterance_based).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EolG4xlX1htI"
      },
      "source": [
        "# importing libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "import os\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# global vars\n",
        "modality_and_repr_type = {'text':['sentence-BERT',512], 'video':['resnet101',2048], 'audio':['opensmile',65]}\n",
        "text_size = modality_and_repr_type['text'][1]\n",
        "visual_size = modality_and_repr_type['video'][1]\n",
        "acoustic_size = modality_and_repr_type['audio'][1]\n",
        "RANDOM_SEED = 42\n",
        "BATCH_SIZE = 4\n",
        "LR = 1e-04\n",
        "EPOCHS = 50\n",
        "\n",
        "# seeds for reproducibility\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBWunOhThTGg"
      },
      "source": [
        "# declaring train and test dataframes\n",
        "df_train = pd.DataFrame()\n",
        "df_test = pd.DataFrame()\n",
        "\n",
        "if 'text' in modality_and_repr_type: \n",
        "  !gdown --id 1Z-Mt2kMtA6ZJ5YA704SRqFtCZsNbPMMR -q # train raw\n",
        "  !gdown --id 1TWO58qHYYEVcUeaEltVv2xVH1HLVsUnK -q # test raw\n",
        "  df_train = pd.read_csv('train.tsv',sep='\\t')\n",
        "  df_test = pd.read_csv('test.tsv',sep='\\t')\n",
        "  print(\"Downloaded the raw text M2H2 data !\")\n",
        "\n",
        "  # download feature representations\n",
        "  if modality_and_repr_type['text'][0]=='sentence-BERT':\n",
        "    !gdown --id 1decZ9lPDjxlKLJZfpvKW8vUynz6UA01n -q # train\n",
        "    !gdown --id 1--0jt4tgOGRfajVMYBHuoKKDw-aZjgUW -q # test\n",
        "    df_train['text'] = pd.DataFrame({'text':np.loadtxt('train_utterance_embeddings_sentenceBERT.txt').tolist()})\n",
        "    df_test['text'] = pd.DataFrame({'text':np.loadtxt('test_utterance_embeddings_sentenceBERT.txt').tolist()})\n",
        "    print(\"Downloaded the sentence-BERT embeddings !\")\n",
        "  elif modality_and_repr_type['text'][0]=='FastText':\n",
        "    !gdown --id 1CP9Q83PQ1eD6D3QpTxQQWhdmpZ4Bb70r -q # train\n",
        "    !gdown --id 11-89-yI6uwslPACgqsTNMxjTOV_LzNK1 -q # test\n",
        "    df_train['text'] = pd.DataFrame({'text':np.loadtxt('train_utterance_embeddings_FastText.txt').tolist()})\n",
        "    df_test['text'] = pd.DataFrame({'text':np.loadtxt('test_utterance_embeddings_FastText.txt').tolist()})\n",
        "    print(\"Downloaded the FastText embeddings !\")\n",
        "\n",
        "if 'video' in modality_and_repr_type:\n",
        "  !gdown --id 1Z-Mt2kMtA6ZJ5YA704SRqFtCZsNbPMMR -q # train raw\n",
        "  !gdown --id 1TWO58qHYYEVcUeaEltVv2xVH1HLVsUnK -q # test raw\n",
        "  df_train['Label'] = np.array(pd.read_csv('train.tsv',sep='\\t')['Label'])\n",
        "  df_test['Label'] = np.array(pd.read_csv('test.tsv',sep='\\t')['Label'])\n",
        "  print(\"Downloaded the raw text M2H2 data !\")\n",
        "\n",
        "  !gdown --id 1J0cc2mf2n03zAGwbLZ9TO1SEHtWAs9Rb -q # train\n",
        "  !gdown --id 191WO9nVckQnjbiAy3NROZXQOId5AX_w9 -q # test\n",
        "  df_train['video'] = pd.DataFrame({'video':np.loadtxt('train_utterance_features_resnext101.txt').tolist()})\n",
        "  df_test['video'] = pd.DataFrame({'video':np.loadtxt('test_utterance_features_resnext101.txt').tolist()})\n",
        "  print(\"Downloaded the resnect101 features !\")\n",
        "\n",
        "if 'audio' in modality_and_repr_type:\n",
        "  !gdown --id 1Z-Mt2kMtA6ZJ5YA704SRqFtCZsNbPMMR -q # train raw\n",
        "  !gdown --id 1TWO58qHYYEVcUeaEltVv2xVH1HLVsUnK -q # test raw\n",
        "  df_train['Label'] = np.array(pd.read_csv('train.tsv',sep='\\t')['Label'])\n",
        "  df_test['Label'] = np.array(pd.read_csv('test.tsv',sep='\\t')['Label'])\n",
        "  print(\"Downloaded the raw text M2H2 data !\")\n",
        "\n",
        "  !gdown --id 1-2isFu4OFEpg4ftrcdpOeHNrRCJ9OLPo -q # train\n",
        "  !gdown --id 1-GlUVqGL4oLtYzfz7Ik1HuzMiGLAGUL3 -q # test\n",
        "  df_train['audio'] = pd.DataFrame({'audio':np.loadtxt('train_features_opensmile_avg.txt').tolist()})\n",
        "  df_test['audio'] = pd.DataFrame({'audio':np.loadtxt('test_features_opensmile_avg.txt').tolist()})\n",
        "  print(\"Downloaded the opensmile averaged features !\")\n",
        "\n",
        "''' Vanilla data split'''\n",
        "# train-val split\n",
        "df_train, df_val = train_test_split(\n",
        "  df_train,\n",
        "  test_size=0.1,\n",
        "  random_state=RANDOM_SEED\n",
        ")\n",
        "print(\"Number of utterances in train : \", len(df_train))\n",
        "print(\"Number of utterances in val : \", len(df_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUsPWoA4iIE8"
      },
      "source": [
        "# creating dataset\n",
        "class M2H2_Dataset(Dataset):\n",
        "  def __init__(self, df, modality_and_repr_type):\n",
        "    self.len = len(df)\n",
        "    self.text_features = []\n",
        "    self.video_features = []\n",
        "    self.audio_features = []\n",
        "    self.labels = []   \n",
        "\n",
        "    # load the features depending on the modality \n",
        "    if 'text' in modality_and_repr_type.keys():\n",
        "      self.text_features = np.array(list(df['text']))\n",
        "    if 'video' in modality_and_repr_type.keys():\n",
        "      self.video_features = np.array(list(df['video']))\n",
        "    if 'audio' in modality_and_repr_type.keys():\n",
        "      self.audio_features = np.array(list(df['audio']))\n",
        "\n",
        "    self.labels = np.array(list(df['Label']))\n",
        "\n",
        "  def __len__(self):\n",
        "    # data length\n",
        "    return self.len\n",
        "  def __getitem__(self, index):\n",
        "     # return one item based on the index value\n",
        "     sample = {'textf': torch.from_numpy(self.text_features[index]),\n",
        "               'audiof':torch.from_numpy(self.audio_features[index]), \n",
        "               'videof':torch.from_numpy(self.video_features[index]),\n",
        "               'labels':torch.from_numpy(np.array(self.labels[index]))}\n",
        "     return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOTPjEBZidJB"
      },
      "source": [
        "# creating data loader\n",
        "def create_data_loader(df, batch_size):\n",
        "  ds = M2H2_Dataset(df,modality_and_repr_type)\n",
        "  return DataLoader(ds,batch_size=batch_size,num_workers=2)\n",
        "\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY3EfM8yl4dv"
      },
      "source": [
        "## MISA code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y7x94Lcl4JT"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Function\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class ReverseLayerF(Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, p):\n",
        "        ctx.p = p\n",
        "\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        print(\"dekha hai pahli baar\")\n",
        "        output = grad_output.neg() * ctx.p\n",
        "\n",
        "        return output, None\n",
        "\n",
        "\n",
        "def to_gpu(x, on_cpu=False, gpu_id=None):\n",
        "    \"\"\"Tensor => Variable\"\"\"\n",
        "    if torch.cuda.is_available() and not on_cpu:\n",
        "        x = x.cuda(gpu_id)\n",
        "    return x\n",
        "\n",
        "\n",
        "def masked_mean(tensor, mask, dim):\n",
        "    \"\"\"Finding the mean along dim\"\"\"\n",
        "    masked = torch.mul(tensor, mask)\n",
        "    return masked.sum(dim=dim) / mask.sum(dim=dim)\n",
        "\n",
        "def masked_max(tensor, mask, dim):\n",
        "    \"\"\"Finding the max along dim\"\"\"\n",
        "    masked = torch.mul(tensor, mask)\n",
        "    neg_inf = torch.zeros_like(tensor)\n",
        "    neg_inf[~mask] = -math.inf\n",
        "    return (masked + neg_inf).max(dim=dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8Izz0SOl8L-"
      },
      "source": [
        "# let's define a simple model that can deal with multimodal variable length sequence\n",
        "class MISA(nn.Module):\n",
        "    def __init__(self, text_tensor,visual_size,acoustic_size):\n",
        "        super(MISA, self).__init__()\n",
        "\n",
        "        self.text_size = text_tensor\n",
        "        self.visual_size = visual_size\n",
        "        self.acoustic_size = acoustic_size\n",
        "\n",
        "\n",
        "        self.input_sizes = input_sizes = [self.text_size, self.visual_size, self.acoustic_size]\n",
        "        self.hidden_sizes = hidden_sizes = [int(self.text_size), int(self.visual_size), int(self.acoustic_size)]\n",
        "        self.output_size = output_size = 7\n",
        "        self.dropout_rate = dropout_rate = 0.03\n",
        "        self.activation = nn.ELU()\n",
        "        self.tanh = nn.Tanh()\n",
        "        \n",
        "        \n",
        "        rnn =  nn.GRU\n",
        "        # defining modules - two layer bidirectional LSTM with layer norm in between\n",
        "\n",
        "        \n",
        "        #self.embed = nn.Embedding(len(config.word2id), input_sizes[0])\n",
        "        self.trnn1 = rnn(input_sizes[0], hidden_sizes[0], bidirectional=True)\n",
        "        self.trnn2 = rnn(2*hidden_sizes[0], hidden_sizes[0], bidirectional=True)\n",
        "        \n",
        "        self.vrnn1 = rnn(input_sizes[1], hidden_sizes[1], bidirectional=True)\n",
        "        self.vrnn2 = rnn(2*hidden_sizes[1], hidden_sizes[1], bidirectional=True)\n",
        "        \n",
        "        self.arnn1 = rnn(input_sizes[2], hidden_sizes[2], bidirectional=True)\n",
        "        self.arnn2 = rnn(2*hidden_sizes[2], hidden_sizes[2], bidirectional=True)\n",
        "\n",
        "\n",
        "\n",
        "        ##########################################\n",
        "        # mapping modalities to same sized space\n",
        "        ##########################################\n",
        "       \n",
        "        self.project_t = nn.Sequential()\n",
        "        self.project_t.add_module('project_t', nn.Linear(in_features=hidden_sizes[0], out_features=512))\n",
        "        self.project_t.add_module('project_t_activation', self.activation)\n",
        "        self.project_t.add_module('project_t_layer_norm', nn.LayerNorm(512))\n",
        "        \n",
        "\n",
        "        self.project_v = nn.Sequential()\n",
        "        self.project_v.add_module('project_v', nn.Linear(in_features=hidden_sizes[1], out_features=512))\n",
        "        self.project_v.add_module('project_v_activation', self.activation)\n",
        "        self.project_v.add_module('project_v_layer_norm', nn.LayerNorm(512))\n",
        "\n",
        "        self.project_a = nn.Sequential()\n",
        "        self.project_a.add_module('project_a', nn.Linear(in_features=hidden_sizes[2], out_features=512))\n",
        "        self.project_a.add_module('project_a_activation', self.activation)\n",
        "        self.project_a.add_module('project_a_layer_norm', nn.LayerNorm(512))\n",
        "\n",
        "\n",
        "        ##########################################\n",
        "        # private encoders\n",
        "        ##########################################\n",
        "        self.private_t = nn.Sequential()\n",
        "        self.private_t.add_module('private_t_1', nn.Linear(in_features=512, out_features=512))\n",
        "        self.private_t.add_module('private_t_activation_1', nn.Sigmoid())\n",
        "        \n",
        "        self.private_v = nn.Sequential()\n",
        "        self.private_v.add_module('private_v_1', nn.Linear(in_features=512, out_features=512))\n",
        "        self.private_v.add_module('private_v_activation_1', nn.Sigmoid())\n",
        "        \n",
        "        self.private_a = nn.Sequential()\n",
        "        self.private_a.add_module('private_a_3', nn.Linear(in_features=512, out_features=512))\n",
        "        self.private_a.add_module('private_a_activation_3', nn.Sigmoid())\n",
        "        \n",
        "\n",
        "        ##########################################\n",
        "        # shared encoder\n",
        "        ##########################################\n",
        "        self.shared = nn.Sequential()\n",
        "        self.shared.add_module('shared_1', nn.Linear(in_features=512, out_features=512))\n",
        "        self.shared.add_module('shared_activation_1', nn.Sigmoid())\n",
        "\n",
        "\n",
        "        ##########################################\n",
        "        # reconstruct\n",
        "        ##########################################\n",
        "        self.recon_t = nn.Sequential()\n",
        "        self.recon_t.add_module('recon_t_1', nn.Linear(in_features=512, out_features=512))\n",
        "        self.recon_v = nn.Sequential()\n",
        "        self.recon_v.add_module('recon_v_1', nn.Linear(in_features=512, out_features=512))\n",
        "        self.recon_a = nn.Sequential()\n",
        "        self.recon_a.add_module('recon_a_1', nn.Linear(in_features=512, out_features=512))\n",
        "\n",
        "\n",
        "\n",
        "        ##########################################\n",
        "        # shared space adversarial discriminator\n",
        "        ##########################################\n",
        "        self.discriminator = nn.Sequential()\n",
        "        self.discriminator.add_module('discriminator_layer_1', nn.Linear(in_features=512, out_features=512))\n",
        "        self.discriminator.add_module('discriminator_layer_1_activation', self.activation)\n",
        "        self.discriminator.add_module('discriminator_layer_1_dropout', nn.Dropout(dropout_rate))\n",
        "        self.discriminator.add_module('discriminator_layer_2', nn.Linear(in_features=512, out_features=len(hidden_sizes)))\n",
        "\n",
        "        ##########################################\n",
        "        # shared-private collaborative discriminator\n",
        "        ##########################################\n",
        "\n",
        "        self.sp_discriminator = nn.Sequential()\n",
        "        self.sp_discriminator.add_module('sp_discriminator_layer_1', nn.Linear(in_features=512, out_features=4))\n",
        "\n",
        "        self.tlayer_norm = nn.LayerNorm((hidden_sizes[0]*2,))\n",
        "        self.vlayer_norm = nn.LayerNorm((hidden_sizes[1]*2,))\n",
        "        self.alayer_norm = nn.LayerNorm((hidden_sizes[2]*2,))\n",
        "\n",
        "        encoder_layer = torch.nn.TransformerEncoderLayer(d_model=512, nhead=2) # 128 is the default hidden size used by MISA\n",
        "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
        "\n",
        "    #original MISA\n",
        "    def extract_features(self, sequence, lengths, rnn1, rnn2, layer_norm):\n",
        "        packed_h1, final_h1 = rnn1(sequence)\n",
        "        normed_h1 = layer_norm(packed_h1)\n",
        "        _, final_h2 = rnn2(normed_h1)\n",
        "\n",
        "        return final_h1, final_h2\n",
        "\n",
        "    def alignment(self, sentences, visual, acoustic, lengths):\n",
        "        \n",
        "        self.shared_private(sentences, visual, acoustic)\n",
        "        \n",
        "        #misa original\n",
        "        #self.shared_private(utterance_text, utterance_video, utterance_audio)\n",
        "\n",
        "        \n",
        "        reversed_shared_code_t = ReverseLayerF.apply(self.utt_shared_t, 1.0)\n",
        "        reversed_shared_code_v = ReverseLayerF.apply(self.utt_shared_v, 1.0)\n",
        "        reversed_shared_code_a = ReverseLayerF.apply(self.utt_shared_a, 1.0)\n",
        "\n",
        "        self.domain_label_t = self.discriminator(reversed_shared_code_t)\n",
        "        self.domain_label_v = self.discriminator(reversed_shared_code_v)\n",
        "        self.domain_label_a = self.discriminator(reversed_shared_code_a)\n",
        "        \n",
        "\n",
        "\n",
        "        self.shared_or_private_p_t = self.sp_discriminator(self.utt_private_t)\n",
        "        self.shared_or_private_p_v = self.sp_discriminator(self.utt_private_v)\n",
        "        self.shared_or_private_p_a = self.sp_discriminator(self.utt_private_a)\n",
        "        self.shared_or_private_s = self.sp_discriminator( (self.utt_shared_t + self.utt_shared_v + self.utt_shared_a)/3.0 )\n",
        "        \n",
        "        # For reconstruction\n",
        "        self.reconstruct()\n",
        "        \n",
        "        # 1-LAYER TRANSFORMER FUSION\n",
        "        h = torch.stack((self.utt_private_t, self.utt_private_v, self.utt_private_a, self.utt_shared_t, self.utt_shared_v,  self.utt_shared_a), dim=0)\n",
        "        \n",
        "        h = self.transformer_encoder(h)\n",
        "        h = torch.cat((h[0], h[1], h[2], h[3], h[4], h[5]), dim=1)\n",
        "        #print(h)\n",
        "        return h\n",
        "    \n",
        "    def reconstruct(self,):\n",
        "\n",
        "        self.utt_t = (self.utt_private_t + self.utt_shared_t)\n",
        "        self.utt_v = (self.utt_private_v + self.utt_shared_v)\n",
        "        self.utt_a = (self.utt_private_a + self.utt_shared_a)\n",
        "        \n",
        "\n",
        "        self.utt_t_recon = self.recon_t(self.utt_t)\n",
        "        self.utt_v_recon = self.recon_v(self.utt_v)\n",
        "        self.utt_a_recon = self.recon_a(self.utt_a)\n",
        "\n",
        "\n",
        "    def shared_private(self, utterance_t, utterance_v, utterance_a):\n",
        "        \n",
        "        # Projecting to same sized space\n",
        "        self.utt_t_orig = utterance_t = self.project_t(utterance_t)\n",
        "        self.utt_v_orig = utterance_v = self.project_v(utterance_v)\n",
        "        self.utt_a_orig = utterance_a = self.project_a(utterance_a)\n",
        "        \n",
        "        # Private-shared components\n",
        "        self.utt_private_t = self.private_t(utterance_t)\n",
        "        self.utt_private_v = self.private_v(utterance_v)\n",
        "        self.utt_private_a = self.private_a(utterance_a)\n",
        "\n",
        "        self.utt_shared_t = self.shared(utterance_t)\n",
        "        self.utt_shared_v = self.shared(utterance_v)\n",
        "        self.utt_shared_a = self.shared(utterance_a)\n",
        "\n",
        "\n",
        "    def forward(self, sentences, video, acoustic, lengths):\n",
        "        o = self.alignment(sentences, video, acoustic, lengths)\n",
        "        return o"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE3Xb13sl9YG"
      },
      "source": [
        "## Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhof-kXZiiKE"
      },
      "source": [
        "class HumorClassifier(nn.Module):\n",
        "  def __init__(self, n_classes, feature_len):\n",
        "    super(HumorClassifier, self).__init__()\n",
        "    self.layer1 = nn.Linear(feature_len, 2048)\n",
        "    self.layer2 = nn.Linear(2048, 4096)\n",
        "    self.layer3 = nn.Linear(4096, 2048)\n",
        "    self.layer4 = nn.Linear(2048, 1024)\n",
        "    self.layer5 = nn.Linear(1024,n_classes)\n",
        "    self.dropout = nn.Dropout(p=0.1)\n",
        "    self.relu = nn.ReLU()\n",
        "  def forward(self, x):\n",
        "    x = self.relu(self.layer1(x))\n",
        "    x = self.relu(self.layer2(x))\n",
        "    x = self.relu(self.layer3(x))\n",
        "    x = self.relu(self.layer4(x))\n",
        "    x = self.layer5(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0YKTpaJWX_R"
      },
      "source": [
        "## Defining classifier and MISA model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjiIuVA2ikVX"
      },
      "source": [
        "# instantiate the MISA model\n",
        "misa_model = MISA(text_size,visual_size,acoustic_size).to(device)\n",
        "\n",
        "# define the NN\n",
        "model = HumorClassifier(2,feature_len=3072) \n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4XKPgsxmeLq"
      },
      "source": [
        "## Losses for MISA model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vbmGimMmguM"
      },
      "source": [
        "class MSE(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MSE, self).__init__()\n",
        "\n",
        "  def forward(self, pred, real):\n",
        "    diffs = torch.add(real, -pred)\n",
        "    n = torch.numel(diffs.data)\n",
        "    mse = torch.sum(diffs.pow(2)) / n\n",
        "    return mse\n",
        "\n",
        "\n",
        "class DiffLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DiffLoss, self).__init__()\n",
        "\n",
        "  def forward(self, input1, input2):\n",
        "\n",
        "    batch_size = input1.size(0)\n",
        "    input1 = input1.view(batch_size, -1)\n",
        "    input2 = input2.view(batch_size, -1)\n",
        "\n",
        "    # Zero mean\n",
        "    input1_mean = torch.mean(input1, dim=0, keepdims=True)\n",
        "    input2_mean = torch.mean(input2, dim=0, keepdims=True)\n",
        "    input1 = input1 - input1_mean\n",
        "    input2 = input2 - input2_mean\n",
        "\n",
        "    input1_l2_norm = torch.norm(input1, p=2, dim=1, keepdim=True).detach()\n",
        "    input1_l2 = input1.div(input1_l2_norm.expand_as(input1) + 1e-6)\n",
        "    \n",
        "\n",
        "    input2_l2_norm = torch.norm(input2, p=2, dim=1, keepdim=True).detach()\n",
        "    input2_l2 = input2.div(input2_l2_norm.expand_as(input2) + 1e-6)\n",
        "\n",
        "    diff_loss = torch.mean((input1_l2.t().mm(input2_l2)).pow(2))\n",
        "\n",
        "    return diff_loss\n",
        "\n",
        "\n",
        "class CMD(nn.Module):\n",
        "  \"\"\"\n",
        "  Adapted from https://github.com/wzell/cmd/blob/master/models/domain_regularizer.py\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super(CMD, self).__init__()\n",
        "\n",
        "  def forward(self, x1, x2, n_moments):\n",
        "    mx1 = torch.mean(x1, 0)\n",
        "    mx2 = torch.mean(x2, 0)\n",
        "    sx1 = x1-mx1\n",
        "    sx2 = x2-mx2\n",
        "    dm = self.matchnorm(mx1, mx2)\n",
        "    scms = dm\n",
        "    for i in range(n_moments - 1):\n",
        "        scms += self.scm(sx1, sx2, i + 2)\n",
        "    return scms\n",
        "\n",
        "  def matchnorm(self, x1, x2):\n",
        "    power = torch.pow(x1-x2,2)\n",
        "    summed = torch.sum(power)\n",
        "    sqrt = summed**(0.5)\n",
        "    return sqrt\n",
        "    # return ((x1-x2)**2).sum().sqrt()\n",
        "\n",
        "  def scm(self, sx1, sx2, k):\n",
        "    ss1 = torch.mean(torch.pow(sx1, k), 0)\n",
        "    ss2 = torch.mean(torch.pow(sx2, k), 0)\n",
        "    return self.matchnorm(ss1, ss2)\n",
        "\n",
        "# reconstruction loss\n",
        "def get_recon_loss(misa_model,loss_recon):\n",
        "  loss =  loss_recon(misa_model.utt_t_recon, misa_model.utt_t_orig)\n",
        "  loss += loss_recon(misa_model.utt_v_recon, misa_model.utt_v_orig)\n",
        "  loss += loss_recon(misa_model.utt_a_recon, misa_model.utt_a_orig)\n",
        "  loss = loss/3.0\n",
        "  return loss\n",
        "\n",
        "\n",
        "def get_cmd_loss(misa_model,loss_cmd):\n",
        "  # losses between shared states\n",
        "  loss = loss_cmd(misa_model.utt_shared_t, misa_model.utt_shared_v, 5)\n",
        "  loss += loss_cmd(misa_model.utt_shared_t, misa_model.utt_shared_a, 5)\n",
        "  loss += loss_cmd(misa_model.utt_shared_a, misa_model.utt_shared_v, 5)\n",
        "  loss = loss/3.0\n",
        "\n",
        "  return loss\n",
        "\n",
        "    \n",
        "def get_diff_loss(misa_model, loss_diff):\n",
        "  shared_t = misa_model.utt_shared_t\n",
        "  shared_v = misa_model.utt_shared_v\n",
        "  shared_a = misa_model.utt_shared_a\n",
        "  private_t = misa_model.utt_private_t\n",
        "  private_v = misa_model.utt_private_v\n",
        "  private_a = misa_model.utt_private_a\n",
        "\n",
        "  # Between private and shared\n",
        "  loss = loss_diff(private_t, shared_t)\n",
        "  loss += loss_diff(private_v, shared_v)\n",
        "  loss += loss_diff(private_a, shared_a)\n",
        "\n",
        "  # Across privates\n",
        "  loss += loss_diff(private_a, private_t)\n",
        "  loss += loss_diff(private_a, private_v)\n",
        "  loss += loss_diff(private_t, private_v)\n",
        "\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1D5mKU_ImlZm"
      },
      "source": [
        "# optimizer and loss function definition\n",
        "optimizer = torch.optim.Adam(list(model.parameters())+list(misa_model.parameters()), lr=LR) #1e-4 works #, weight_decay = 0.01) # both model parameters need to be trained\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "loss_recon = MSE()\n",
        "loss_cmd = CMD()\n",
        "loss_diff = DiffLoss()\n",
        "loss_fn = FocalLoss(gamma=2).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9LkPhPvjVig"
      },
      "source": [
        "## Train and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seyq9iJ3i1Kl"
      },
      "source": [
        "def train_epoch(model,\n",
        "                misa_model,\n",
        "                data_loader,\n",
        "                loss_fn,\n",
        "                loss_recon,\n",
        "                loss_cmd,\n",
        "                loss_diff,\n",
        "                optimizer,\n",
        "                device,\n",
        "                n_examples):\n",
        "  \n",
        "  model = model.train()\n",
        "  misa_model = misa_model.train()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  # getting the fused representation\n",
        "  for i, data in enumerate(data_loader): \n",
        "    textf, acouf,visuf,targets = data['textf'],data['audiof'],data['videof'],data['labels'].long()\n",
        "    textf,acouf,visuf,targets = textf.to(device),acouf.to(device),visuf.to(device),targets.to(device)  \n",
        "    fused_repr = misa_model(textf.float(),visuf.float(),acouf.float(),len(textf)) # shape: (bs,Z)\n",
        "    outputs = model(fused_repr)\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    # task loss\n",
        "    loss = loss_fn(outputs, targets)\n",
        "    \n",
        "    # different losses for training MISA\n",
        "    recon_loss = get_recon_loss(misa_model,loss_recon)\n",
        "    diff_loss = get_diff_loss(misa_model,loss_diff)\n",
        "    similarity_loss = get_cmd_loss(misa_model,loss_cmd) \n",
        "\n",
        "    total_loss = loss + recon_loss + diff_loss + similarity_loss \n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(total_loss.item())\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SVCVj0ai4Um"
      },
      "source": [
        "def eval_model(model, misa_model, data_loader, loss_fn, loss_recon, loss_cmd, loss_diff, device, n_examples):\n",
        "  model = model.eval()\n",
        "  misa_model = misa_model.eval()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  with torch.no_grad():\n",
        "    for i,data in enumerate(data_loader,0):\n",
        "      textf, acouf,visuf,targets = data['textf'],data['audiof'],data['videof'],data['labels'].long()\n",
        "      textf,acouf,visuf,targets = textf.to(device),acouf.to(device),visuf.to(device),targets.to(device)  \n",
        "      fused_repr = misa_model(textf.float(),visuf.float(),acouf.float(),len(textf)) # shape: (bs,Z)\n",
        "      outputs = model(fused_repr)\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      # task loss\n",
        "      loss = loss_fn(outputs, targets)\n",
        "      # different losses for training MISA\n",
        "      recon_loss = get_recon_loss(misa_model,loss_recon)\n",
        "      diff_loss = get_diff_loss(misa_model,loss_diff)\n",
        "      similarity_loss = get_cmd_loss(misa_model,loss_cmd) # default option in MISA code (https://github.com/declare-lab/MISA/blob/ec42faddde0d210cf7368aebf2118fe9570e7102/src/config.py#L81)\n",
        "\n",
        "      total_loss = loss + recon_loss + diff_loss + similarity_loss \n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(total_loss.item())\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWPV0SMDi_Ip"
      },
      "source": [
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    misa_model,\n",
        "    train_data_loader,\n",
        "    loss_fn,\n",
        "    loss_recon,\n",
        "    loss_cmd,\n",
        "    loss_diff,\n",
        "    optimizer,\n",
        "    device,\n",
        "    len(df_train))\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    misa_model,\n",
        "    val_data_loader,\n",
        "    loss_fn,\n",
        "    loss_recon,\n",
        "    loss_cmd,\n",
        "    loss_diff,\n",
        "    device,\n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  scheduler.step(val_loss)\n",
        "\n",
        "  print(f'Val loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "    best_accuracy = val_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTyUOMSuWxVy"
      },
      "source": [
        "## Results(F1, P and R values)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEJ7rlhj19Wm"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  review_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "  with torch.no_grad():\n",
        "    for i,data in enumerate(data_loader,0):\n",
        "      textf, acouf,visuf,targets = data['textf'],data['audiof'],data['videof'],data['labels'].long()\n",
        "      textf,acouf,visuf = textf.to(device),acouf.to(device),visuf.to(device)  \n",
        "      fused_repr = misa_model(textf.float(),visuf.float(),acouf.float(),len(textf)) # shape: (6,bs,seq_len, Z)\n",
        "      outputs = model(fused_repr)\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(outputs)\n",
        "      real_values.extend(targets)\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return review_texts, predictions, prediction_probs, real_values\n",
        "\n",
        "# load the best model\n",
        "model.load_state_dict(torch.load('best_model_state.bin'))\n",
        "# get results on test set\n",
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ykc9Ijs9_hl"
      },
      "source": [
        "print('Stats for ',modality_and_repr_type,'\\n')\n",
        "class_names = ['not humorous', 'humorous']\n",
        "print(classification_report(y_test, y_pred, target_names=class_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lbf1yRFx-GfL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}