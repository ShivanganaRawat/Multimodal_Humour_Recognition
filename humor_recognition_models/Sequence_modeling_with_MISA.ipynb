{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Sequence modeling with MISA",
      "provenance": [],
      "collapsed_sections": [
        "qzN0DWheSzOG",
        "kMTpM6_HSTtL",
        "Gwz2Y9ksSZO7",
        "rzYaMsoCNK6H"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_Kp9SRo61m5"
      },
      "source": [
        "# Contributors: \n",
        "* Shivangana Rawat(cs20mtech12001) </br>\n",
        "* Pranoy Panda(cs20mtech12002) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzN0DWheSzOG"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn0jVJnVDS3u"
      },
      "source": [
        "# deep learning framework\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# for loading and manipulating data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "\n",
        "# for visualization\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "\n",
        "# for splitting data and evaluation metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# miscellaneous\n",
        "import os\n",
        "import random\n",
        "\n",
        "#define global vars\n",
        "RANDOM_SEED = 42\n",
        "EPOCHS = 50\n",
        "LR = 5e-07\n",
        "BATCH_SIZE = 4\n",
        "SEQLEN = 5\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "modality_and_repr_type = {'text':['sentence-BERT',512], 'video':['resnet101',2048], 'audio':['opensmile',65]}\n",
        "text_size = modality_and_repr_type['text'][1]\n",
        "visual_size = modality_and_repr_type['video'][1]\n",
        "acoustic_size = modality_and_repr_type['audio'][1]\n",
        "\n",
        "# setting seeds for reproducibility\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMTpM6_HSTtL"
      },
      "source": [
        "## Download datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oPkduyKFraz"
      },
      "source": [
        "# declaring train and test dataframes\n",
        "df_train = pd.DataFrame()\n",
        "df_test = pd.DataFrame()\n",
        "\n",
        "if 'text' in modality_and_repr_type: \n",
        "  !gdown --id 1Z-Mt2kMtA6ZJ5YA704SRqFtCZsNbPMMR -q # train raw\n",
        "  !gdown --id 1TWO58qHYYEVcUeaEltVv2xVH1HLVsUnK -q # test raw\n",
        "  df_train = pd.read_csv('train.tsv',sep='\\t')\n",
        "  df_test = pd.read_csv('test.tsv',sep='\\t')\n",
        "  print(\"Downloaded the raw text M2H2 data !\")\n",
        "\n",
        "  # download feature representations\n",
        "  if modality_and_repr_type['text'][0]=='sentence-BERT':\n",
        "    !gdown --id 1decZ9lPDjxlKLJZfpvKW8vUynz6UA01n -q # train\n",
        "    !gdown --id 1--0jt4tgOGRfajVMYBHuoKKDw-aZjgUW -q # test\n",
        "    df_train['text'] = pd.DataFrame({'text':np.loadtxt('train_utterance_embeddings_sentenceBERT.txt').tolist()})\n",
        "    df_test['text'] = pd.DataFrame({'text':np.loadtxt('test_utterance_embeddings_sentenceBERT.txt').tolist()})\n",
        "    print(\"Downloaded the sentence-BERT embeddings !\")\n",
        "  elif modality_and_repr_type['text'][0]=='FastText':\n",
        "    !gdown --id 1CP9Q83PQ1eD6D3QpTxQQWhdmpZ4Bb70r -q # train\n",
        "    !gdown --id 11-89-yI6uwslPACgqsTNMxjTOV_LzNK1 -q # test\n",
        "    df_train['text'] = pd.DataFrame({'text':np.loadtxt('train_utterance_embeddings_FastText.txt').tolist()})\n",
        "    df_test['text'] = pd.DataFrame({'text':np.loadtxt('test_utterance_embeddings_FastText.txt').tolist()})\n",
        "    print(\"Downloaded the FastText embeddings !\")\n",
        "\n",
        "if 'video' in modality_and_repr_type:\n",
        "  !gdown --id 1Z-Mt2kMtA6ZJ5YA704SRqFtCZsNbPMMR -q # train raw\n",
        "  !gdown --id 1TWO58qHYYEVcUeaEltVv2xVH1HLVsUnK -q # test raw\n",
        "  df_train['Label'] = np.array(pd.read_csv('train.tsv',sep='\\t')['Label'])\n",
        "  df_test['Label'] = np.array(pd.read_csv('test.tsv',sep='\\t')['Label'])\n",
        "  print(\"Downloaded the raw text M2H2 data !\")\n",
        "\n",
        "  !gdown --id 1J0cc2mf2n03zAGwbLZ9TO1SEHtWAs9Rb -q # train\n",
        "  !gdown --id 191WO9nVckQnjbiAy3NROZXQOId5AX_w9 -q # test\n",
        "  df_train['video'] = pd.DataFrame({'video':np.loadtxt('train_utterance_features_resnext101.txt').tolist()})\n",
        "  df_test['video'] = pd.DataFrame({'video':np.loadtxt('test_utterance_features_resnext101.txt').tolist()})\n",
        "  print(\"Downloaded the resnect101 features !\")\n",
        "\n",
        "if 'audio' in modality_and_repr_type:\n",
        "  !gdown --id 1Z-Mt2kMtA6ZJ5YA704SRqFtCZsNbPMMR -q # train raw\n",
        "  !gdown --id 1TWO58qHYYEVcUeaEltVv2xVH1HLVsUnK -q # test raw\n",
        "  df_train['Label'] = np.array(pd.read_csv('train.tsv',sep='\\t')['Label'])\n",
        "  df_test['Label'] = np.array(pd.read_csv('test.tsv',sep='\\t')['Label'])\n",
        "  print(\"Downloaded the raw text M2H2 data !\")\n",
        "\n",
        "  !gdown --id 1-2isFu4OFEpg4ftrcdpOeHNrRCJ9OLPo -q # train\n",
        "  !gdown --id 1-GlUVqGL4oLtYzfz7Ik1HuzMiGLAGUL3 -q # test\n",
        "  df_train['audio'] = pd.DataFrame({'audio':np.loadtxt('train_features_opensmile_avg.txt').tolist()})\n",
        "  df_test['audio'] = pd.DataFrame({'audio':np.loadtxt('test_features_opensmile_avg.txt').tolist()})\n",
        "  print(\"Downloaded the opensmile averaged features !\")\n",
        "\n",
        "''' Vanilla data split'''\n",
        "# train-val split\n",
        "df_train, df_val = train_test_split(\n",
        "  df_train,\n",
        "  test_size=0.1,\n",
        "  random_state=RANDOM_SEED\n",
        ")\n",
        "print(\"Number of utterances in train : \", len(df_train))\n",
        "print(\"Number of utterances in val : \", len(df_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwz2Y9ksSZO7"
      },
      "source": [
        "## Create scene+episode wise split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh7SiMlAFwrC"
      },
      "source": [
        "# creating scenewise train val split\n",
        "df_train['Episode Scene'] = df_train['episode'] + df_train['Scenes']\n",
        "df_test['Episode Scene'] = df_test['episode'] + df_test['Scenes']\n",
        "ep_scene = list(set(df_train['Episode Scene']))\n",
        "random.shuffle(ep_scene)\n",
        "trainlist = ep_scene[:int(0.9*len(ep_scene))]\n",
        "testlist = ep_scene[int(0.9*len(ep_scene)):]\n",
        "\n",
        "traindf = pd.DataFrame()\n",
        "for epsc in trainlist:\n",
        "  tempdf = df_train.loc[df_train['Episode Scene'] == epsc]\n",
        "  traindf = pd.concat([traindf, tempdf])\n",
        "valdf = pd.DataFrame()\n",
        "for epsc in testlist:\n",
        "  tempdf = df_train.loc[df_train['Episode Scene'] == epsc]\n",
        "  valdf = pd.concat([valdf, tempdf])\n",
        "df_train = traindf\n",
        "df_val = valdf\n",
        "\n",
        "# sorting rows based on episodes and scenes\n",
        "df_train.sort_values(by=['episode', 'Scenes'])\n",
        "df_val.sort_values(by=['episode', 'Scenes'])\n",
        "df_test.sort_values(by=['episode', 'Scenes'])\n",
        "\n",
        "print(\"Number of utterances in train : \", len(df_train))\n",
        "print(\"Number of utterances in val : \", len(df_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LodItc5ftSKk"
      },
      "source": [
        "## Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts7zVo3sFzyC"
      },
      "source": [
        "# creating dataset\n",
        "class M2H2_Dataset(Dataset):\n",
        "  def __init__(self, df, modality_and_repr_type, seqlen):\n",
        "    self.df = df\n",
        "    self.text_features = torch.from_numpy(np.array(list(df['text'])))\n",
        "    self.video_features = torch.from_numpy(np.array(list(df['video'])))\n",
        "    self.audio_features = torch.from_numpy(np.array(list(df['audio']))) # now using audio as text\n",
        "\n",
        "    index = 0\n",
        "    indexlist = []\n",
        "    for i in range(len(self.df['Episode Scene'])):\n",
        "      # value_counts() Return a Series containing counts of unique rows in the DataFrame.\n",
        "      if len(self.df['Episode Scene'][i: i+seqlen].value_counts()) == 1 and self.df['Episode Scene'][i: i+seqlen].value_counts()[0] == seqlen:\n",
        "        indexlist.append(index)\n",
        "        index= index+1\n",
        "      else:\n",
        "        indexlist.append(None)\n",
        "    self.df['Index'] = indexlist\n",
        "    self.labels = torch.from_numpy(np.array(list(df['Label'])))\n",
        "    self.df = self.df.reset_index()\n",
        "    self.len = self.df['Index'].max()\n",
        "    self.seqlen = seqlen\n",
        "\n",
        "  def __len__(self):\n",
        "    # data length\n",
        "    return int(self.len)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "     # return one item based on the index value\n",
        "     idx = self.df.index[self.df['Index'] == index]\n",
        "     indexes = list(range(idx[0], idx[0]+self.seqlen))\n",
        "     samples = {'textf':self.text_features[indexes],\n",
        "                'videof':self.video_features[indexes],\n",
        "                'audiof':self.audio_features[indexes],\n",
        "                'labels':self.labels[indexes]}\n",
        "     return samples"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pcBZd20F1Qf"
      },
      "source": [
        "# creating data loader\n",
        "def create_data_loader(df, batch_size, seqlen,dataset_class):\n",
        "  ds = dataset_class(df,modality_and_repr_type, seqlen)\n",
        "  return DataLoader(ds,batch_size=batch_size,num_workers=2)\n",
        "\n",
        "\n",
        "dataset_class = M2H2_Dataset\n",
        "train_data_loader = create_data_loader(df_train, BATCH_SIZE, SEQLEN,dataset_class)\n",
        "val_data_loader = create_data_loader(df_val, BATCH_SIZE, SEQLEN,dataset_class)\n",
        "test_data_loader = create_data_loader(df_test, BATCH_SIZE, SEQLEN,dataset_class)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzYaMsoCNK6H"
      },
      "source": [
        "## MISA Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DaRLs8ML_es"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Function\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class ReverseLayerF(Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, p):\n",
        "        ctx.p = p\n",
        "\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        print(\"dekha hai pahli baar\")\n",
        "        output = grad_output.neg() * ctx.p\n",
        "\n",
        "        return output, None\n",
        "\n",
        "\n",
        "def to_gpu(x, on_cpu=False, gpu_id=None):\n",
        "    \"\"\"Tensor => Variable\"\"\"\n",
        "    if torch.cuda.is_available() and not on_cpu:\n",
        "        x = x.cuda(gpu_id)\n",
        "    return x\n",
        "\n",
        "\n",
        "def masked_mean(tensor, mask, dim):\n",
        "    \"\"\"Finding the mean along dim\"\"\"\n",
        "    masked = torch.mul(tensor, mask)\n",
        "    return masked.sum(dim=dim) / mask.sum(dim=dim)\n",
        "\n",
        "def masked_max(tensor, mask, dim):\n",
        "    \"\"\"Finding the max along dim\"\"\"\n",
        "    masked = torch.mul(tensor, mask)\n",
        "    neg_inf = torch.zeros_like(tensor)\n",
        "    neg_inf[~mask] = -math.inf\n",
        "    return (masked + neg_inf).max(dim=dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8HzZnBXNRN9"
      },
      "source": [
        "# let's define a simple model that can deal with multimodal variable length sequence\n",
        "class MISA(nn.Module):\n",
        "    def __init__(self, text_tensor,visual_size,acoustic_size):\n",
        "        super(MISA, self).__init__()\n",
        "\n",
        "        self.text_size = text_tensor\n",
        "        self.visual_size = visual_size\n",
        "        self.acoustic_size = acoustic_size\n",
        "\n",
        "\n",
        "        self.input_sizes = input_sizes = [self.text_size, self.visual_size, self.acoustic_size]\n",
        "        self.hidden_sizes = hidden_sizes = [int(self.text_size), int(self.visual_size), int(self.acoustic_size)]\n",
        "        self.output_size = output_size = 7\n",
        "        self.dropout_rate = dropout_rate = 0.03\n",
        "        self.activation = nn.ELU()\n",
        "        self.tanh = nn.Tanh()\n",
        "        \n",
        "        \n",
        "        rnn =  nn.GRU\n",
        "        # defining modules - two layer bidirectional LSTM with layer norm in between\n",
        "\n",
        "        \n",
        "        #self.embed = nn.Embedding(len(config.word2id), input_sizes[0])\n",
        "        self.trnn1 = rnn(input_sizes[0], hidden_sizes[0], bidirectional=True)\n",
        "        self.trnn2 = rnn(2*hidden_sizes[0], hidden_sizes[0], bidirectional=True)\n",
        "        \n",
        "        self.vrnn1 = rnn(input_sizes[1], hidden_sizes[1], bidirectional=True)\n",
        "        self.vrnn2 = rnn(2*hidden_sizes[1], hidden_sizes[1], bidirectional=True)\n",
        "        \n",
        "        self.arnn1 = rnn(input_sizes[2], hidden_sizes[2], bidirectional=True)\n",
        "        self.arnn2 = rnn(2*hidden_sizes[2], hidden_sizes[2], bidirectional=True)\n",
        "\n",
        "\n",
        "\n",
        "        ##########################################\n",
        "        # mapping modalities to same sized space\n",
        "        ##########################################\n",
        "       \n",
        "        self.project_t = nn.Sequential()\n",
        "        self.project_t.add_module('project_t', nn.Linear(in_features=hidden_sizes[0], out_features=512))\n",
        "        self.project_t.add_module('project_t_activation', self.activation)\n",
        "        self.project_t.add_module('project_t_layer_norm', nn.LayerNorm(512))\n",
        "        \n",
        "\n",
        "        self.project_v = nn.Sequential()\n",
        "        self.project_v.add_module('project_v', nn.Linear(in_features=hidden_sizes[1], out_features=512))\n",
        "        self.project_v.add_module('project_v_activation', self.activation)\n",
        "        self.project_v.add_module('project_v_layer_norm', nn.LayerNorm(512))\n",
        "\n",
        "        self.project_a = nn.Sequential()\n",
        "        self.project_a.add_module('project_a', nn.Linear(in_features=hidden_sizes[2], out_features=512))\n",
        "        self.project_a.add_module('project_a_activation', self.activation)\n",
        "        self.project_a.add_module('project_a_layer_norm', nn.LayerNorm(512))\n",
        "\n",
        "\n",
        "        ##########################################\n",
        "        # private encoders\n",
        "        ##########################################\n",
        "        self.private_t = nn.Sequential()\n",
        "        self.private_t.add_module('private_t_1', nn.Linear(in_features=512, out_features=512))\n",
        "        self.private_t.add_module('private_t_activation_1', nn.Sigmoid())\n",
        "        \n",
        "        self.private_v = nn.Sequential()\n",
        "        self.private_v.add_module('private_v_1', nn.Linear(in_features=512, out_features=512))\n",
        "        self.private_v.add_module('private_v_activation_1', nn.Sigmoid())\n",
        "        \n",
        "        self.private_a = nn.Sequential()\n",
        "        self.private_a.add_module('private_a_3', nn.Linear(in_features=512, out_features=512))\n",
        "        self.private_a.add_module('private_a_activation_3', nn.Sigmoid())\n",
        "        \n",
        "\n",
        "        ##########################################\n",
        "        # shared encoder\n",
        "        ##########################################\n",
        "        self.shared = nn.Sequential()\n",
        "        self.shared.add_module('shared_1', nn.Linear(in_features=512, out_features=512))\n",
        "        self.shared.add_module('shared_activation_1', nn.Sigmoid())\n",
        "\n",
        "\n",
        "        ##########################################\n",
        "        # reconstruct\n",
        "        ##########################################\n",
        "        self.recon_t = nn.Sequential()\n",
        "        self.recon_t.add_module('recon_t_1', nn.Linear(in_features=512, out_features=512))\n",
        "        self.recon_v = nn.Sequential()\n",
        "        self.recon_v.add_module('recon_v_1', nn.Linear(in_features=512, out_features=512))\n",
        "        self.recon_a = nn.Sequential()\n",
        "        self.recon_a.add_module('recon_a_1', nn.Linear(in_features=512, out_features=512))\n",
        "\n",
        "\n",
        "\n",
        "        ##########################################\n",
        "        # shared space adversarial discriminator\n",
        "        ##########################################\n",
        "        self.discriminator = nn.Sequential()\n",
        "        self.discriminator.add_module('discriminator_layer_1', nn.Linear(in_features=512, out_features=512))\n",
        "        self.discriminator.add_module('discriminator_layer_1_activation', self.activation)\n",
        "        self.discriminator.add_module('discriminator_layer_1_dropout', nn.Dropout(dropout_rate))\n",
        "        self.discriminator.add_module('discriminator_layer_2', nn.Linear(in_features=512, out_features=len(hidden_sizes)))\n",
        "\n",
        "        ##########################################\n",
        "        # shared-private collaborative discriminator\n",
        "        ##########################################\n",
        "\n",
        "        self.sp_discriminator = nn.Sequential()\n",
        "        self.sp_discriminator.add_module('sp_discriminator_layer_1', nn.Linear(in_features=512, out_features=4))\n",
        "\n",
        "        self.tlayer_norm = nn.LayerNorm((hidden_sizes[0]*2,))\n",
        "        self.vlayer_norm = nn.LayerNorm((hidden_sizes[1]*2,))\n",
        "        self.alayer_norm = nn.LayerNorm((hidden_sizes[2]*2,))\n",
        "\n",
        "        encoder_layer = torch.nn.TransformerEncoderLayer(d_model=3072, nhead=2) # 128 is the default hidden size used by MISA\n",
        "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
        "\n",
        "    #original MISA\n",
        "    def extract_features(self, sequence, lengths, rnn1, rnn2, layer_norm):\n",
        "        packed_h1, final_h1 = rnn1(sequence)\n",
        "        normed_h1 = layer_norm(packed_h1)\n",
        "        _, final_h2 = rnn2(normed_h1)\n",
        "\n",
        "        return final_h1, final_h2\n",
        "\n",
        "    def alignment(self, sentences, visual, acoustic, lengths):\n",
        "        \n",
        "        self.shared_private(sentences, visual, acoustic)\n",
        "        \n",
        "        #misa original\n",
        "        #self.shared_private(utterance_text, utterance_video, utterance_audio)\n",
        "\n",
        "        \n",
        "        reversed_shared_code_t = ReverseLayerF.apply(self.utt_shared_t, 1.0)\n",
        "        reversed_shared_code_v = ReverseLayerF.apply(self.utt_shared_v, 1.0)\n",
        "        reversed_shared_code_a = ReverseLayerF.apply(self.utt_shared_a, 1.0)\n",
        "\n",
        "        self.domain_label_t = self.discriminator(reversed_shared_code_t)\n",
        "        self.domain_label_v = self.discriminator(reversed_shared_code_v)\n",
        "        self.domain_label_a = self.discriminator(reversed_shared_code_a)\n",
        "        \n",
        "\n",
        "\n",
        "        self.shared_or_private_p_t = self.sp_discriminator(self.utt_private_t)\n",
        "        self.shared_or_private_p_v = self.sp_discriminator(self.utt_private_v)\n",
        "        self.shared_or_private_p_a = self.sp_discriminator(self.utt_private_a)\n",
        "        self.shared_or_private_s = self.sp_discriminator( (self.utt_shared_t + self.utt_shared_v + self.utt_shared_a)/3.0 )\n",
        "        \n",
        "        # For reconstruction\n",
        "        self.reconstruct()\n",
        "        \n",
        "        # 1-LAYER TRANSFORMER FUSION\n",
        "        h = torch.stack((self.utt_private_t, self.utt_private_v, self.utt_private_a, self.utt_shared_t, self.utt_shared_v,  self.utt_shared_a), dim=0) # dims: (6,bs,seq_len,512)\n",
        "        h = h.transpose(0,1) # dims: (bs,6,seq_len,512)\n",
        "        h = h.transpose(1,2) # dims: (bs,seq_len,6,512)\n",
        "        # print('after stacking: ',h.shape)\n",
        "        h = torch.reshape(h,(h.size(0),h.size(1),h.size(2)*h.size(3)))\n",
        "        # print('after flatenning: ', h.shape)\n",
        "        h = self.transformer_encoder(h)\n",
        "        # print('h after transformer encoder: ',h.shape)\n",
        "        return h\n",
        "    \n",
        "    def reconstruct(self,):\n",
        "\n",
        "        self.utt_t = (self.utt_private_t + self.utt_shared_t)\n",
        "        self.utt_v = (self.utt_private_v + self.utt_shared_v)\n",
        "        self.utt_a = (self.utt_private_a + self.utt_shared_a)\n",
        "        \n",
        "\n",
        "        self.utt_t_recon = self.recon_t(self.utt_t)\n",
        "        self.utt_v_recon = self.recon_v(self.utt_v)\n",
        "        self.utt_a_recon = self.recon_a(self.utt_a)\n",
        "\n",
        "\n",
        "    def shared_private(self, utterance_t, utterance_v, utterance_a):\n",
        "        \n",
        "        # Projecting to same sized space\n",
        "        self.utt_t_orig = utterance_t = self.project_t(utterance_t)\n",
        "        self.utt_v_orig = utterance_v = self.project_v(utterance_v)\n",
        "        self.utt_a_orig = utterance_a = self.project_a(utterance_a)\n",
        "        \n",
        "        # Private-shared components\n",
        "        self.utt_private_t = self.private_t(utterance_t)\n",
        "        self.utt_private_v = self.private_v(utterance_v)\n",
        "        self.utt_private_a = self.private_a(utterance_a)\n",
        "\n",
        "        self.utt_shared_t = self.shared(utterance_t)\n",
        "        self.utt_shared_v = self.shared(utterance_v)\n",
        "        self.utt_shared_a = self.shared(utterance_a)\n",
        "\n",
        "\n",
        "    def forward(self, sentences, video, acoustic, lengths):\n",
        "        o = self.alignment(sentences, video, acoustic, lengths)\n",
        "        return o"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYD4XY6iNp0v"
      },
      "source": [
        "## Training and Testing function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwwCeC_-Nvf1"
      },
      "source": [
        "def train_epoch(model,\n",
        "                misa_model,\n",
        "                data_loader,\n",
        "                loss_fn,\n",
        "                loss_recon,\n",
        "                loss_cmd,\n",
        "                loss_diff,\n",
        "                optimizer,\n",
        "                device,\n",
        "                n_examples):\n",
        "  \n",
        "  model = model.train()\n",
        "  misa_model = misa_model.train()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  \n",
        "  for i, data in enumerate(data_loader): \n",
        "    textf, acouf,visuf,targets = data['textf'],data['audiof'],data['videof'],data['labels']\n",
        "    textf,acouf,visuf,targets = textf.to(device),acouf.to(device),visuf.to(device),targets.to(device) \n",
        "    # getting the fused representation \n",
        "    fused_repr = misa_model(textf.float(),visuf.float(),acouf.float(),len(textf)) # shape: (bs,seq_len, Z)\n",
        "    outputs = model(fused_repr)\n",
        "    targets = targets[:, -1].long()\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    # compute task loss\n",
        "    loss = loss_fn(outputs, targets)\n",
        "    \n",
        "    # compute different losses for training MISA\n",
        "    recon_loss = get_recon_loss(misa_model,loss_recon)\n",
        "    diff_loss = get_diff_loss(misa_model,loss_diff)\n",
        "    similarity_loss = get_cmd_loss(misa_model,loss_cmd) # default option in MISA code (https://github.com/declare-lab/MISA/blob/ec42faddde0d210cf7368aebf2118fe9570e7102/src/config.py#L81)\n",
        "\n",
        "    # final loss to backprop\n",
        "    total_loss = loss + recon_loss + diff_loss + similarity_loss \n",
        "    \n",
        "    #book keeping\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(total_loss.item())\n",
        "    \n",
        "    # backprop\n",
        "    total_loss.backward() # compute gradients for all params via chain rule\n",
        "    optimizer.step() # update parameters via Adam\n",
        "    optimizer.zero_grad() # clear accumulated gradients\n",
        "    \n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3ZJlPplPWJa"
      },
      "source": [
        "def eval_model(model, misa_model, data_loader, loss_fn, loss_recon, loss_cmd, loss_diff, device, n_examples):\n",
        "  model = model.eval()\n",
        "  misa_model = misa_model.eval()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  with torch.no_grad():\n",
        "    for i,data in enumerate(data_loader,0):\n",
        "      textf, acouf,visuf,targets = data['textf'],data['audiof'],data['videof'],data['labels']\n",
        "      textf,acouf,visuf,targets = textf.to(device),acouf.to(device),visuf.to(device),targets.to(device)  \n",
        "      fused_repr = misa_model(textf.float(),visuf.float(),acouf.float(),len(textf)) # shape: (bs,seq_len, Z)\n",
        "      outputs = model(fused_repr)\n",
        "      targets = targets[:, -1].long()\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      # task loss\n",
        "      loss = loss_fn(outputs, targets)\n",
        "      # different losses for training MISA\n",
        "      recon_loss = get_recon_loss(misa_model,loss_recon)\n",
        "      diff_loss = get_diff_loss(misa_model,loss_diff)\n",
        "      similarity_loss = get_cmd_loss(misa_model,loss_cmd) # default option in MISA code (https://github.com/declare-lab/MISA/blob/ec42faddde0d210cf7368aebf2118fe9570e7102/src/config.py#L81)\n",
        "\n",
        "      total_loss = loss + recon_loss + diff_loss + similarity_loss \n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(total_loss.item())\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd0XIBLHN2h6"
      },
      "source": [
        "## Declaring the SequenceClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbUcXFo3N64Y"
      },
      "source": [
        "class SequenceHumorClassifier(nn.Module):\n",
        "  def __init__(self, n_classes, feature_len):\n",
        "    super(SequenceHumorClassifier, self).__init__()\n",
        "    self.lstm = nn.LSTM(feature_len, 512, proj_size=128, batch_first=True) # input shape: [batch_size, time_steps,feature_size]\n",
        "    self.classifier = nn.Sequential(\n",
        "                                nn.Linear(128,512),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(512,1024),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(1024,512),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(512,256),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(256,n_classes)\n",
        "                                )\n",
        "  def forward(self, x):\n",
        "    out, (hn, cn) = self.lstm(x)\n",
        "    out = self.classifier(out[:, -1, :])\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNlUNvQr124T"
      },
      "source": [
        "## Defining the sequence classifier and MISA models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoEki3gf10uG"
      },
      "source": [
        "model = SequenceHumorClassifier(2,feature_len=3072) \n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "# instantiate the MISA object\n",
        "misa_model = MISA(text_size,visual_size,acoustic_size).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL-bqBEatexR"
      },
      "source": [
        "## Function for different losses of MISA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7exzpVY6kG93"
      },
      "source": [
        "class MSE(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MSE, self).__init__()\n",
        "\n",
        "  def forward(self, pred, real):\n",
        "    diffs = torch.add(real, -pred)\n",
        "    n = torch.numel(diffs.data)\n",
        "    mse = torch.sum(diffs.pow(2)) / n\n",
        "    return mse\n",
        "\n",
        "\n",
        "class DiffLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DiffLoss, self).__init__()\n",
        "\n",
        "  def forward(self, input1, input2):\n",
        "\n",
        "    batch_size = input1.size(0)\n",
        "    input1 = input1.view(batch_size, -1)\n",
        "    input2 = input2.view(batch_size, -1)\n",
        "\n",
        "    # Zero mean\n",
        "    input1_mean = torch.mean(input1, dim=0, keepdims=True)\n",
        "    input2_mean = torch.mean(input2, dim=0, keepdims=True)\n",
        "    input1 = input1 - input1_mean\n",
        "    input2 = input2 - input2_mean\n",
        "\n",
        "    input1_l2_norm = torch.norm(input1, p=2, dim=1, keepdim=True).detach()\n",
        "    input1_l2 = input1.div(input1_l2_norm.expand_as(input1) + 1e-6)\n",
        "    \n",
        "\n",
        "    input2_l2_norm = torch.norm(input2, p=2, dim=1, keepdim=True).detach()\n",
        "    input2_l2 = input2.div(input2_l2_norm.expand_as(input2) + 1e-6)\n",
        "\n",
        "    diff_loss = torch.mean((input1_l2.t().mm(input2_l2)).pow(2))\n",
        "\n",
        "    return diff_loss\n",
        "\n",
        "\n",
        "class CMD(nn.Module):\n",
        "  \"\"\"\n",
        "  Adapted from https://github.com/wzell/cmd/blob/master/models/domain_regularizer.py\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super(CMD, self).__init__()\n",
        "\n",
        "  def forward(self, x1, x2, n_moments):\n",
        "    mx1 = torch.mean(x1, 0)\n",
        "    mx2 = torch.mean(x2, 0)\n",
        "    sx1 = x1-mx1\n",
        "    sx2 = x2-mx2\n",
        "    dm = self.matchnorm(mx1, mx2)\n",
        "    scms = dm\n",
        "    for i in range(n_moments - 1):\n",
        "        scms += self.scm(sx1, sx2, i + 2)\n",
        "    return scms\n",
        "\n",
        "  def matchnorm(self, x1, x2):\n",
        "    power = torch.pow(x1-x2,2)\n",
        "    summed = torch.sum(power)\n",
        "    sqrt = summed**(0.5)\n",
        "    return sqrt\n",
        "    # return ((x1-x2)**2).sum().sqrt()\n",
        "\n",
        "  def scm(self, sx1, sx2, k):\n",
        "    ss1 = torch.mean(torch.pow(sx1, k), 0)\n",
        "    ss2 = torch.mean(torch.pow(sx2, k), 0)\n",
        "    return self.matchnorm(ss1, ss2)\n",
        "\n",
        "# reconstruction loss\n",
        "def get_recon_loss(misa_model,loss_recon):\n",
        "  loss =  loss_recon(misa_model.utt_t_recon, misa_model.utt_t_orig)\n",
        "  loss += loss_recon(misa_model.utt_v_recon, misa_model.utt_v_orig)\n",
        "  loss += loss_recon(misa_model.utt_a_recon, misa_model.utt_a_orig)\n",
        "  loss = loss/3.0\n",
        "  return loss\n",
        "\n",
        "\n",
        "def get_cmd_loss(misa_model,loss_cmd):\n",
        "  # losses between shared states\n",
        "  loss = loss_cmd(misa_model.utt_shared_t, misa_model.utt_shared_v, 5)\n",
        "  loss += loss_cmd(misa_model.utt_shared_t, misa_model.utt_shared_a, 5)\n",
        "  loss += loss_cmd(misa_model.utt_shared_a, misa_model.utt_shared_v, 5)\n",
        "  loss = loss/3.0\n",
        "\n",
        "  return loss\n",
        "\n",
        "    \n",
        "def get_diff_loss(misa_model, loss_diff):\n",
        "  shared_t = misa_model.utt_shared_t\n",
        "  shared_v = misa_model.utt_shared_v\n",
        "  shared_a = misa_model.utt_shared_a\n",
        "  private_t = misa_model.utt_private_t\n",
        "  private_v = misa_model.utt_private_v\n",
        "  private_a = misa_model.utt_private_a\n",
        "\n",
        "  # Between private and shared\n",
        "  loss = loss_diff(private_t, shared_t)\n",
        "  loss += loss_diff(private_v, shared_v)\n",
        "  loss += loss_diff(private_a, shared_a)\n",
        "\n",
        "  # Across privates\n",
        "  loss += loss_diff(private_a, private_t)\n",
        "  loss += loss_diff(private_a, private_v)\n",
        "  loss += loss_diff(private_t, private_v)\n",
        "\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75R1OoDM2Dxc"
      },
      "source": [
        "## Setting up optimizer and losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVM5-4VzOCBx"
      },
      "source": [
        "optimizer = torch.optim.Adam(list(model.parameters())+list(misa_model.parameters()), lr=LR)#, weight_decay = 0.1) # both model parameters need to be trained\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "# MISA losses\n",
        "loss_recon = MSE()\n",
        "loss_cmd = CMD()\n",
        "loss_diff = DiffLoss()\n",
        "# task loss\n",
        "loss_fn = nn.CrossEntropyLoss().to(device) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJj20Qa4tpGq"
      },
      "source": [
        "## Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4Hp1OHKQOHEK"
      },
      "source": [
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    misa_model,\n",
        "    train_data_loader,\n",
        "    loss_fn,\n",
        "    loss_recon,\n",
        "    loss_cmd,\n",
        "    loss_diff,\n",
        "    optimizer,\n",
        "    device,\n",
        "    len(df_train))\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    misa_model,\n",
        "    val_data_loader,\n",
        "    loss_fn,\n",
        "    loss_recon,\n",
        "    loss_cmd,\n",
        "    loss_diff,\n",
        "    device,\n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  scheduler.step(val_loss)\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "    best_accuracy = val_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCbCAWe4tt_K"
      },
      "source": [
        "## Results(F1, P and R values along with confusion matrix)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-qqVnjIhLMQq"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  review_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "  with torch.no_grad():\n",
        "    for i,data in enumerate(data_loader,0):\n",
        "      textf, acouf,visuf,targets = data['textf'],data['audiof'],data['videof'],data['labels']\n",
        "      textf,acouf,visuf = textf.to(device),acouf.to(device),visuf.to(device)  \n",
        "      fused_repr = misa_model(textf.float(),visuf.float(),acouf.float(),len(textf)) # shape: (bs,seq_len, Z)\n",
        "      outputs = model(fused_repr)\n",
        "      targets = targets[:, -1].long()\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(outputs)\n",
        "      real_values.extend(targets)\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return review_texts, predictions, prediction_probs, real_values\n",
        "\n",
        "\n",
        "model.load_state_dict(torch.load('best_model_state.bin'))\n",
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "k8CD3GY3LOZB"
      },
      "source": [
        "print('Stats for ',modality_and_repr_type,'\\n')\n",
        "class_names = ['not humorous', 'humorous']\n",
        "print(classification_report(y_test, y_pred, target_names=class_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "07XOGEVDLRL7"
      },
      "source": [
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a7iREvu9zACJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}