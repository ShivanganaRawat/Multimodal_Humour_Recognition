{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multimodal Humour Recognition(simple concatenation).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FApXWIYoMKgY"
      },
      "source": [
        "## M2H2 dataset\n",
        "1. Raw text data- </br>\n",
        "(a) Train(train.tsv) - gdrive id: 1Z-Mt2kMtA6ZJ5YA704SRqFtCZsNbPMMR </br>\n",
        "(b) Test(test.tsv) - gdrive id: 1TWO58qHYYEVcUeaEltVv2xVH1HLVsUnK</br>\n",
        "\n",
        "2. Sentence-BERT features(per utterance 512 dims) - </br>\n",
        "(a) Train(train_utterance_embeddings_sentenceBERT.txt) - gdrive id: 1decZ9lPDjxlKLJZfpvKW8vUynz6UA01n</br>\n",
        "(b) Test(test_utterance_embeddings_sentenceBERT.txt) - gdrive id: 1--0jt4tgOGRfajVMYBHuoKKDw-aZjgUW\n",
        "\n",
        "3. FastText features(per utterance 300 dims) - </br>\n",
        "(a) Train(train_utterance_embeddings_FastText.txt) - gdrive id: 1CP9Q83PQ1eD6D3QpTxQQWhdmpZ4Bb70r </br>\n",
        "(b) Test(test_utterance_embeddings_FastText.txt) - gdrive id: 11-89-yI6uwslPACgqsTNMxjTOV_LzNK1\n",
        "\n",
        "4. 3D CNN(ResNext101) features(per utterance 2048 dims) - </br>\n",
        "(a) Train(train_utterance_features_resnext101.txt) - gdrive id: 1J0cc2mf2n03zAGwbLZ9TO1SEHtWAs9Rb </br>\n",
        "(b) Test(test_utterance_features_resnext101.txt) - gdrive id: 191WO9nVckQnjbiAy3NROZXQOId5AX_w9\n",
        "\n",
        "5. openSmile features(per utterance 65 dims) - </br>\n",
        "(a) Train(train_features_opensmile_avg.txt) - gdrive id: \n",
        "1-2isFu4OFEpg4ftrcdpOeHNrRCJ9OLPo </br>\n",
        "(b) Test(test_features_opensmile_avg.txt) - gdrive id: \n",
        "1-GlUVqGL4oLtYzfz7Ik1HuzMiGLAGUL3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBIko6viQbzX"
      },
      "source": [
        "# importing libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "import os\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "# set parameters\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "\n",
        "#define vars\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oepiBRk_lzU8"
      },
      "source": [
        "'''\n",
        "'text':['FastText',300]\n",
        "'text':['sentence-BERT',512]\n",
        "'''\n",
        "#modality_and_repr_type = {'text':['sentence-BERT',512]}\n",
        "#modality_and_repr_type = {'video':['resnet101',2048]}\n",
        "modality_and_repr_type = {'audio':['opensmile',65]}\n",
        "#modality_and_repr_type = {'text':['sentence-BERT',512], 'video':['resnet101',2048]}\n",
        "#modality_and_repr_type = {'text':['sentence-BERT',512], 'audio':['opensmile',65]}\n",
        "#modality_and_repr_type = {'video':['resnet101',2048], 'audio':['opensmile',65]}\n",
        "#modality_and_repr_type = {'text':['sentence-BERT',512], 'video':['resnet101',2048], 'audio':['opensmile',65]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygdkwUX5QxxQ"
      },
      "source": [
        "# creating dataset\n",
        "class M2H2_Dataset(Dataset):\n",
        "  def __init__(self, df, modality_and_repr_type):\n",
        "    features = np.array([])\n",
        "    # load the features depending on the modality \n",
        "    if 'text' in modality_and_repr_type.keys():\n",
        "      features = np.array(list(df['text']))\n",
        "    if 'video' in modality_and_repr_type.keys():\n",
        "      if len(features)==0: features = np.array(list(df['video']))\n",
        "      else: features = np.hstack((features,np.array(list(df['video']))))\n",
        "    if 'audio' in modality_and_repr_type.keys():\n",
        "      if len(features)==0: features = np.array(list(df['audio']))\n",
        "      else: features = np.hstack((features,np.array(list(df['audio']))))\n",
        "\n",
        "    self.len = features.shape[0] # number of data points in the dataset\n",
        "    self.x_data = torch.from_numpy(features)\n",
        "    self.y_data = torch.from_numpy(np.array(list(df['Label'])))\n",
        "    \n",
        "  def __len__(self):\n",
        "    # data length\n",
        "    return self.len\n",
        "  def __getitem__(self, index):\n",
        "     # return one item based on the index value\n",
        "     return self.x_data[index],self.y_data[index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7vA2-0FQ1gP"
      },
      "source": [
        "# declaring train and test dataframes\n",
        "df_train = pd.DataFrame()\n",
        "df_test = pd.DataFrame()\n",
        "\n",
        "if 'text' in modality_and_repr_type: \n",
        "  !gdown --id 1Z-Mt2kMtA6ZJ5YA704SRqFtCZsNbPMMR -q # train raw\n",
        "  !gdown --id 1TWO58qHYYEVcUeaEltVv2xVH1HLVsUnK -q # test raw\n",
        "  df_train['Label'] = np.array(pd.read_csv('train.tsv',sep='\\t')['Label'])\n",
        "  df_test['Label'] = np.array(pd.read_csv('test.tsv',sep='\\t')['Label'])\n",
        "  print(\"Downloaded the raw text M2H2 data !\")\n",
        "\n",
        "  # download feature representations\n",
        "  if modality_and_repr_type['text'][0]=='sentence-BERT':\n",
        "    !gdown --id 1decZ9lPDjxlKLJZfpvKW8vUynz6UA01n -q # train\n",
        "    !gdown --id 1--0jt4tgOGRfajVMYBHuoKKDw-aZjgUW -q # test\n",
        "    df_train['text'] = pd.DataFrame({'text':np.loadtxt('train_utterance_embeddings_sentenceBERT.txt').tolist()})\n",
        "    df_test['text'] = pd.DataFrame({'text':np.loadtxt('test_utterance_embeddings_sentenceBERT.txt').tolist()})\n",
        "    print(\"Downloaded the sentence-BERT embeddings !\")\n",
        "  elif modality_and_repr_type['text'][0]=='FastText':\n",
        "    !gdown --id 1CP9Q83PQ1eD6D3QpTxQQWhdmpZ4Bb70r -q # train\n",
        "    !gdown --id 11-89-yI6uwslPACgqsTNMxjTOV_LzNK1 -q # test\n",
        "    df_train['text'] = pd.DataFrame({'text':np.loadtxt('train_utterance_embeddings_FastText.txt').tolist()})\n",
        "    df_test['text'] = pd.DataFrame({'text':np.loadtxt('test_utterance_embeddings_FastText.txt').tolist()})\n",
        "    print(\"Downloaded the FastText embeddings !\")\n",
        "\n",
        "if 'video' in modality_and_repr_type:\n",
        "  !gdown --id 1Z-Mt2kMtA6ZJ5YA704SRqFtCZsNbPMMR -q # train raw\n",
        "  !gdown --id 1TWO58qHYYEVcUeaEltVv2xVH1HLVsUnK -q # test raw\n",
        "  df_train['Label'] = np.array(pd.read_csv('train.tsv',sep='\\t')['Label'])\n",
        "  df_test['Label'] = np.array(pd.read_csv('test.tsv',sep='\\t')['Label'])\n",
        "  print(\"Downloaded the raw text M2H2 data !\")\n",
        "\n",
        "  !gdown --id 1J0cc2mf2n03zAGwbLZ9TO1SEHtWAs9Rb -q # train\n",
        "  !gdown --id 191WO9nVckQnjbiAy3NROZXQOId5AX_w9 -q # test\n",
        "  df_train['video'] = pd.DataFrame({'video':np.loadtxt('train_utterance_features_resnext101.txt').tolist()})\n",
        "  df_test['video'] = pd.DataFrame({'video':np.loadtxt('test_utterance_features_resnext101.txt').tolist()})\n",
        "  print(\"Downloaded the resnect101 features !\")\n",
        "\n",
        "if 'audio' in modality_and_repr_type:\n",
        "  !gdown --id 1Z-Mt2kMtA6ZJ5YA704SRqFtCZsNbPMMR -q # train raw\n",
        "  !gdown --id 1TWO58qHYYEVcUeaEltVv2xVH1HLVsUnK -q # test raw\n",
        "  df_train['Label'] = np.array(pd.read_csv('train.tsv',sep='\\t')['Label'])\n",
        "  df_test['Label'] = np.array(pd.read_csv('test.tsv',sep='\\t')['Label'])\n",
        "  print(\"Downloaded the raw text M2H2 data !\")\n",
        "\n",
        "  !gdown --id 1-2isFu4OFEpg4ftrcdpOeHNrRCJ9OLPo -q # train\n",
        "  !gdown --id 1-GlUVqGL4oLtYzfz7Ik1HuzMiGLAGUL3 -q # test\n",
        "  df_train['audio'] = pd.DataFrame({'audio':np.loadtxt('train_features_opensmile_avg.txt').tolist()})\n",
        "  df_test['audio'] = pd.DataFrame({'audio':np.loadtxt('test_features_opensmile_avg.txt').tolist()})\n",
        "  print(\"Downloaded the opensmile averaged features !\")\n",
        "\n",
        "''' Vanilla data split without class balancing'''\n",
        "# train-test split\n",
        "df_train, df_val = train_test_split(\n",
        "  df_train,\n",
        "  test_size=0.1,\n",
        "  random_state=RANDOM_SEED\n",
        ")\n",
        "print(\"Number of utterances in train : \", len(df_train))\n",
        "print(\"Number of utterances in val : \", len(df_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOfjPFa3Q3Y3"
      },
      "source": [
        "# creating data loader\n",
        "def create_data_loader(df, batch_size):\n",
        "  ds = M2H2_Dataset(df,modality_and_repr_type)\n",
        "  return DataLoader(ds,batch_size=batch_size,num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpQFg4EtQ6Bn"
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "train_data_loader = create_data_loader(df_train, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfFzT32sQ9Tn"
      },
      "source": [
        "class HumorClassifier(nn.Module):\n",
        "  def __init__(self, n_classes, feature_len):\n",
        "    super(HumorClassifier, self).__init__()\n",
        "    self.out = nn.Linear(feature_len, n_classes)\n",
        "  def forward(self, x):\n",
        "    return self.out(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldhgKp3DQ_Zl"
      },
      "source": [
        "# get the total feature length\n",
        "feature_len = 0\n",
        "for modality in modality_and_repr_type.keys(): feature_len+=modality_and_repr_type[modality][1]\n",
        "\n",
        "# define the NN\n",
        "model = HumorClassifier(2,feature_len=feature_len) # as we have just two classes: humorous and not humorous \n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BVdiD1BRBX4"
      },
      "source": [
        "# training\n",
        "EPOCHS = 20\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgElGZ5tRDM6"
      },
      "source": [
        "def train_epoch(\n",
        "  model,\n",
        "  data_loader,\n",
        "  loss_fn,\n",
        "  optimizer,\n",
        "  device,\n",
        "  n_examples\n",
        "):\n",
        "  model = model.train()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  for i,data in enumerate(data_loader,0):\n",
        "    inputs, targets = data\n",
        "    inputs, targets = inputs.float(), targets.long()\n",
        "    inputs, targets = Variable(inputs), Variable(targets)\n",
        "    outputs = model(inputs)\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "    loss.backward()\n",
        "    #nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sV4uQSZRE9_"
      },
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  with torch.no_grad():\n",
        "    for i,data in enumerate(data_loader,0):\n",
        "      inputs, targets = data\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "      inputs, targets = inputs.float(), targets.long()\n",
        "      inputs, targets = Variable(inputs), Variable(targets)\n",
        "      outputs = model(inputs)\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      loss = loss_fn(outputs, targets)\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ5kAl3KRG8C"
      },
      "source": [
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "best_model = model\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    len(df_train)\n",
        "  )\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn,\n",
        "    device,\n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "  if val_acc > best_accuracy:\n",
        "    best_model = model\n",
        "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "    best_accuracy = val_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb-QhMLORJah"
      },
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui47PXXCRMGX"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  review_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "  with torch.no_grad():\n",
        "    for i,data in enumerate(data_loader,0):\n",
        "      inputs, targets = data\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "      inputs, targets = inputs.float(), targets.long()\n",
        "      inputs, targets = Variable(inputs), Variable(targets)\n",
        "      outputs = model(inputs)\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(outputs)\n",
        "      real_values.extend(targets)\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return review_texts, predictions, prediction_probs, real_values\n",
        "\n",
        "\n",
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  best_model,\n",
        "  test_data_loader\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CS661ZsROv4"
      },
      "source": [
        "class_names = ['not humorous', 'humorous']\n",
        "print(classification_report(y_test, y_pred, target_names=class_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnsOfA5NN8q4"
      },
      "source": [
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}